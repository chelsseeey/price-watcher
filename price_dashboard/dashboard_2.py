# -*- coding: utf-8 -*-
"""
ìë™ íƒì§€ ëŒ€ì‹œë³´ë“œ (Agoda / Kayak / Amazon)
- streamlit run dashboard.py
- ./outputs í´ë”ì˜ CSV/Parquet ìë™ ìŠ¤ìº”/ë¡œë”©/ë³‘í•©
- í‘œì¤€ ìŠ¤í‚¤ë§ˆ: site, ts_min, price, (ì„ íƒ)item/region/device/currency
- env = "{REGION}-{DEVICE}" â†’ KR/US Ã— PC/Mobile
- ìƒíƒœ(state)ëŠ” íŒŒì¼ëª… *_1/*_2/*_3/*_4 ë¡œ ì¶”ë¡ :
    _1 = ë¡œê·¸ì¸(L1)
    _2 = ë¡œê·¸ì¸(L1) + ì¥ë°”êµ¬ë‹ˆ(Cart)
    _3 = ë¹„ë¡œê·¸ì¸(L0) + ì¿ í‚¤ì‚­ì œ(Cleared)
    _4 = ë¡œê·¸ì¸(L1) + ì¿ í‚¤ì‚­ì œ(Cleared)
=> ENV(4) Ã— STATE(4) = 16 ë¼ì¸, ê°ê¸° ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ ì¶œë ¥
"""

from __future__ import annotations
from pathlib import Path
from typing import Optional, Dict, Any, List
from urllib.parse import urlparse
import re

import numpy as np
import pandas as pd
import streamlit as st
import altair as alt

# ---------------- í˜ì´ì§€ ì„¤ì • ----------------
st.set_page_config(page_title="Real-Time Visualization", layout="wide")

# ---------------- ê¸°ë³¸ ê²½ë¡œ/íŒ¨í„´ ----------------
DEFAULT_DATA_DIR = Path("./outputs")
SCAN_PATTERNS = ["*.csv", "*.parquet"]
MAX_FILES = 50

# ---------------- ì»¬ëŸ¼ ë§¤í•‘ í›„ë³´ ----------------
SITE_ALIASES = ["site", "platform", "seller", "source", "site_name", "domain"]
ITEM_ALIASES = ["item","product","sku","asin","title","name","product_name","url","path","route","hotel","room","room_name"]
TIME_ALIASES = ["ts_min","ts","timestamp","datetime","time","date","scraped_at","collected_at","created_at","run_at","start_ts","start_time"]
PRICE_ALIASES = ["price","fare","total_price","final_price","amount","value","room_price","deal_price"]
REGION_ALIASES = ["region","country","ip_region","geo","location","loc"]
DEVICE_ALIASES = ["device","ua_device","device_type","user_agent_device"]
CURRENCY_ALIASES = ["currency","ccy","cur","price_currency"]

# ---------------- ìœ í‹¸ ----------------
def clean_cols(cols: List[str]) -> List[str]:
    return [str(c).replace("\ufeff","").strip().lower() for c in cols]

def normalize_dt(s: pd.Series) -> pd.Series:
    if not pd.api.types.is_datetime64_any_dtype(s):
        s = pd.to_datetime(s, errors="coerce", infer_datetime_format=True)
    try:
        if getattr(s.dt, "tz", None) is not None:
            s = s.dt.tz_convert(None)
        s = s.dt.tz_localize(None)
    except Exception:
        pass
    return s

def first_present(df: pd.DataFrame, names: List[str]) -> Optional[str]:
    for n in names:
        if n in df.columns:
            return n
    for c in df.columns:
        if any(n in c for n in names):
            return c
    return None

def infer_site_from_filename_or_url(path: Path, df: pd.DataFrame) -> str:
    name = path.name.lower()
    if "agoda" in name: return "agoda"
    if "kayak" in name: return "kayak"
    if "amazon" in name or "amzn" in name: return "amazon"
    url_col = first_present(df, ["url","link","href"])
    if url_col:
        try:
            host = urlparse(str(df[url_col].dropna().astype(str).iloc[0])).netloc.lower()
            if "agoda" in host: return "agoda"
            if "kayak" in host: return "kayak"
            if "amazon" in host: return "amazon"
        except Exception:
            pass
    return "unknown"

def coerce_datetime(series: pd.Series) -> pd.Series:
    if pd.api.types.is_datetime64_any_dtype(series):
        return normalize_dt(series)
    if pd.api.types.is_numeric_dtype(series):
        s = pd.to_datetime(series, unit="ms", errors="coerce")
        if s.isna().all():
            s = pd.to_datetime(series, unit="s", errors="coerce")
        return normalize_dt(s)
    return normalize_dt(pd.to_datetime(series, errors="coerce", infer_datetime_format=True))

def extract_env_from_site(site_val: str) -> Dict[str,str]:
    s = (site_val or "").upper()
    region = "KR" if ("KR" in s or "KOREA" in s) else ("US" if ("US" in s or "USA" in s) else "")
    device = "mobile" if any(k in s for k in ["MOBILE","PHONE","IOS","ANDROID"]) else ("pc" if ("PC" in s or "DESKTOP" in s) else "")
    return {"region": region, "device": device}

def build_env(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if "region" not in df.columns or "device" not in df.columns:
        extr = df["site"].astype(str).apply(extract_env_from_site)
        if "region" not in df.columns: df["region"] = extr.apply(lambda x: x["region"])
        if "device" not in df.columns: df["device"] = extr.apply(lambda x: x["device"])
    def mk_env(r, d):
        r = (str(r).upper() if pd.notna(r) else "").strip()
        d = (str(d).lower() if pd.notna(d) else "").strip()
        if r and d: return f"{r}-{d.upper()}"
        if r: return r
        if d: return d.upper()
        return "(ENV)"
    df["env"] = [mk_env(r,d) for r,d in zip(df.get("region",""), df.get("device",""))]
    return df

# ----------- ìƒíƒœ(state) ì¶”ë¡  (_1/_2/_3/_4) -----------
#   1 â†’ L1 (ë¡œê·¸ì¸) / 2 â†’ L1+Cart / 3 â†’ L0+Cleared / 4 â†’ L1+Cleared
_STATE_CODE_RE = re.compile(r"(?:_|-)(?:price_)?([1234])(?:\D|$)")
def infer_state_code(name: str) -> str:
    m = _STATE_CODE_RE.search(name.lower())
    return m.group(1) if m else "1"

def state_label_from_code(code: str) -> str:
    code = str(code)
    if code == "2":  return "L1+Cart"
    if code == "3":  return "L0+Cleared"
    if code == "4":  return "L1+Cleared"
    return "L1"

def derive_flags_from_code(code: str) -> Dict[str,str]:
    if code == "2":
        return {"login":"L1","cart":"Cart","cookie_cleared":"NotCleared"}
    if code == "3":
        return {"login":"L0","cart":"NoCart","cookie_cleared":"Cleared"}
    if code == "4":
        return {"login":"L1","cart":"NoCart","cookie_cleared":"Cleared"}
    return {"login":"L1","cart":"NoCart","cookie_cleared":"NotCleared"}

# ---------------- íŒŒì¼ ë¡œë”(í‘œì¤€í™”) ----------------
def load_and_standardize(path: Path) -> pd.DataFrame:
    if path.suffix.lower() == ".parquet":
        df = pd.read_parquet(path)
    else:
        df = pd.read_csv(path, encoding="utf-8", low_memory=False)
    df.columns = clean_cols(df.columns.tolist())

    if "login" in df.columns and "logged_in" not in df.columns:
        df = df.rename(columns={"login":"logged_in"})

    site_col = first_present(df, SITE_ALIASES)
    if site_col and site_col != "site":
        df = df.rename(columns={site_col:"site"})
    if "site" not in df.columns:
        df["site"] = infer_site_from_filename_or_url(path, df)
    df["site"] = df["site"].astype(str)

    item_col = first_present(df, ITEM_ALIASES)
    if item_col and item_col != "item":
        df = df.rename(columns={item_col:"item"})
    if "item" not in df.columns:
        df["item"] = "(default)"

    tcol = first_present(df, TIME_ALIASES)
    if tcol and tcol != "ts_min":
        df["ts_min"] = coerce_datetime(df[tcol])
    elif "ts_min" in df.columns:
        df["ts_min"] = coerce_datetime(df["ts_min"])
    else:
        df["ts_min"] = pd.NaT

    pcol = first_present(df, PRICE_ALIASES)
    if pcol and pcol != "price":
        df = df.rename(columns={pcol:"price"})
    if "price" in df.columns and not np.issubdtype(df["price"].dtype, np.number):
        df["price"] = (
            df["price"].astype(str)
            .str.replace(",", "", regex=False)
            .str.extract(r"([-+]?\d*\.?\d+)")[0]
            .astype(float)
        )

    rcol = first_present(df, REGION_ALIASES)
    if rcol and rcol != "region": df = df.rename(columns={rcol:"region"})
    dcol = first_present(df, DEVICE_ALIASES)
    if dcol and dcol != "device": df = df.rename(columns={dcol:"device"})
    ccol = first_present(df, CURRENCY_ALIASES)
    if ccol and ccol != "currency": df = df.rename(columns={ccol:"currency"})

    df["source"] = path.name

    code = infer_state_code(path.name)
    df["state_code"] = code
    df["state"] = state_label_from_code(code)

    flags = derive_flags_from_code(code)
    for k, v in flags.items():
        if k not in df.columns:
            df[k] = v

    if "ts_min" in df.columns:
        df = df.dropna(subset=["ts_min"])
    if "price" in df.columns:
        df = df.dropna(subset=["price"])
        df = df[df["price"] > 0]

    df = build_env(df)

    if set(["site","item","ts_min"]).issubset(df.columns):
        df = df.sort_values(["site","item","ts_min"])
    return df.reset_index(drop=True)

# ---------------- ë°ì´í„° ìŠ¤ìº” ----------------
@st.cache_data(show_spinner=True, ttl=300)
def load_all_data() -> pd.DataFrame:
    files: List[Path] = []
    if DEFAULT_DATA_DIR.exists():
        for pat in SCAN_PATTERNS:
            files.extend(list(DEFAULT_DATA_DIR.glob(pat)))
    files = sorted(files)[:MAX_FILES]

    if not files:
        st.warning("`./outputs`ì—ì„œ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. CSV/Parquetë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        up = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ (CSV/Parquet, ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)", accept_multiple_files=True, type=["csv","parquet"])
        if not up:
            raise FileNotFoundError("ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        dfs = []
        for upl in up:
            tmp_path = Path(upl.name)
            if upl.name.lower().endswith(".parquet"):
                df = pd.read_parquet(upl)
            else:
                df = pd.read_csv(upl, encoding="utf-8", low_memory=False)
            df.to_csv(".tmp.csv", index=False)  # dummy (ë¡œë” í†µì¼ìš©)
            dfs.append(load_and_standardize(tmp_path))
        return pd.concat(dfs, ignore_index=True)

    dfs = []
    for p in files:
        try:
            dfs.append(load_and_standardize(p))
        except Exception as e:
            st.warning(f"{p.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
    if not dfs:
        raise FileNotFoundError("ì½ì„ ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    return pd.concat(dfs, ignore_index=True)

# ---------------- ì‚¬ì´ë“œë°”/í•„í„° ----------------
def sidebar_controls(df: pd.DataFrame) -> Dict[str,Any]:
    st.sidebar.header("í•„í„°")
    sources = sorted(df["source"].dropna().unique().tolist()) if "source" in df.columns else []
    src_sel = st.sidebar.multiselect("ë°ì´í„° ì†ŒìŠ¤(íŒŒì¼)", options=sources, default=sources)
    view = df[df["source"].isin(src_sel)] if src_sel else df

    sites = sorted(view["site"].dropna().unique().tolist())
    site = st.sidebar.selectbox("ì‚¬ì´íŠ¸", sites, index=0 if sites else 0)

    items = sorted(view.loc[view["site"]==site, "item"].dropna().unique().tolist())
    item = st.sidebar.selectbox("ì•„ì´í…œ", items, index=0 if items else 0)

    if not view.empty and "ts_min" in view.columns:
        min_ts, max_ts = view["ts_min"].min(), view["ts_min"].max()
        st.sidebar.caption(f"ë°ì´í„° ê¸°ê°„: {min_ts:%Y-%m-%d %H:%M} ~ {max_ts:%Y-%m-%d %H:%M}")
    else:
        min_ts = max_ts = pd.Timestamp.now()

    start_date = st.sidebar.date_input("ì‹œì‘ ë‚ ì§œ", value=(max_ts.date() if pd.notna(max_ts) else pd.Timestamp.now().date()))
    use_time = st.sidebar.checkbox("ì‹œì‘ ì‹œê° ì§€ì •", value=False)
    hh = st.sidebar.number_input("ì‹œ", 0, 23, 0, 1) if use_time else 0
    mm = st.sidebar.number_input("ë¶„", 0, 59, 0, 1) if use_time else 0
    start_ts = pd.Timestamp(year=start_date.year, month=start_date.month, day=start_date.day, hour=int(hh), minute=int(mm))

    envs_all = sorted(view.loc[(view["site"]==site) & (view["item"]==item), "env"].dropna().unique().tolist())
    envs_sel = st.sidebar.multiselect("Env", options=envs_all, default=envs_all)

    freq = st.sidebar.selectbox("ë¦¬ìƒ˜í”Œ ê°„ê²©", options=["5min","15min","30min","60min","1D"], index=2)

    return dict(view=view, site=site, item=item, start_ts=start_ts, envs=envs_sel, freq=freq)

def apply_filters(view: pd.DataFrame, cfg: Dict[str,Any]) -> pd.DataFrame:
    start_ts = pd.to_datetime(cfg["start_ts"])
    try: start_ts = start_ts.tz_localize(None)
    except Exception: pass
    sel = (
        (view["site"] == cfg["site"]) &
        (view["item"] == cfg["item"]) &
        (view["ts_min"] >= start_ts) &
        (view["env"].isin(cfg["envs"]))
    )
    out = view.loc[sel].copy()
    out["ts_min"] = normalize_dt(out["ts_min"])
    return out.sort_values("ts_min")

# ---------------- KPI ----------------
def kpi_row(df: pd.DataFrame):
    DEC = 2
    def fmt(x): return f"{x:,.{DEC}f}"
    c1, c2, c3, c4, c5 = st.columns(5)
    if df.empty or "price" not in df.columns or df["price"].notna().sum() == 0:
        c1.metric("ë°ì´í„° ìˆ˜", 0); c2.metric("ìµœì‹  ê°€ê²©", "-"); c3.metric("í‰ê·  ê°€ê²©","-"); c4.metric("ìµœì € ê°€ê²©","-"); c5.metric("ìµœê³  ê°€ê²©","-"); return
    use = df.dropna(subset=["price"]).sort_values("ts_min").copy()
    if "currency" in use.columns and use["currency"].notna().any():
        curr_series = use["currency"].astype(str).str.upper()
    else:
        curr_series = pd.Series(np.where(use["site"].str.lower().eq("amazon"), "USD", "KRW"), index=use.index)
    curr_list = sorted(pd.Series(curr_series).dropna().unique().tolist())
    unit_str = " / ".join(curr_list) if curr_list else ""
    last = use["price"].iloc[-1]; avg = use["price"].mean()
    p_min = use["price"].min(); p_max = use["price"].max()
    t_min = use.loc[use["price"].idxmin(),"ts_min"]; t_max = use.loc[use["price"].idxmax(),"ts_min"]
    c1.metric("ë°ì´í„° ìˆ˜", f"{len(use):,}")
    c2.metric(f"ìµœì‹  ê°€ê²© ({unit_str})", fmt(last))
    c3.metric(f"í‰ê·  ê°€ê²© ({unit_str})", fmt(avg))
    c4.metric(f"ìµœì € ê°€ê²© ({unit_str})", fmt(p_min))
    c5.metric(f"ìµœê³  ê°€ê²© ({unit_str})", fmt(p_max))
    st.caption(f"ìµœì € ì‹œê°: {pd.to_datetime(t_min):%Y-%m-%d %H:%M} Â· ìµœê³  ì‹œê°: {pd.to_datetime(t_max):%Y-%m-%d %H:%M}")

# ---------------- 16 ë¼ì¸ ì‹œê°í™” (ì¸í„°ë™í‹°ë¸Œ) ----------------
def plot_env_state_lines(df: pd.DataFrame, freq: str):
    """
    ENV(4) Ã— STATE(4) = 16 ë¼ì¸ì„ ê°ê¸° ë‹¤ë¥¸ ìƒ‰ìœ¼ë¡œ í‘œì‹œ
    + ì°¨íŠ¸ ë‚´ë¶€ ë“œë˜ê·¸/íœ  ì¸í„°ë™ì…˜, ë²”ë¡€ í† ê¸€, í•˜ë‹¨ ë¸ŒëŸ¬ì‹œë¡œ êµ¬ê°„ ì´ë™
    """
    need = {"ts_min","price","env","state"}
    if df is None or df.empty or not need.issubset(df.columns):
        st.info("í‘œì‹œí•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."); return

    t = df.copy()
    t["ts_min"] = pd.to_datetime(t["ts_min"], errors="coerce")
    t = t.dropna(subset=["ts_min","price"]).sort_values("ts_min")
    if t.empty: 
        st.info("ë¦¬ìƒ˜í”Œ ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤."); return

    t["ENV"] = t["env"].astype(str).str.upper()
    t["STATE"] = t["state"].astype(str)
    t["EnvState"] = t["ENV"] + " | " + t["STATE"]

    long = (
        t.set_index("ts_min")
         .groupby("EnvState")["price"]
         .resample(freq).mean()
         .reset_index()
         .dropna(subset=["price"])
    )
    if long.empty: 
        st.info("ë¦¬ìƒ˜í”Œ ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤."); return

    sel_leg = alt.selection_point(fields=["EnvState"], bind="legend")
    brush = alt.selection_interval(encodings=["x"])

    detail = (
        alt.Chart(long)
          .mark_line()
          .encode(
              x=alt.X("ts_min:T", title="Time"),
              y=alt.Y("price:Q", title="Price"),
              color=alt.Color("EnvState:N", title="Env | State")
          )
          .add_params(sel_leg)
          .transform_filter(sel_leg)
          .transform_filter(brush)
          .properties(height=360)
          .interactive()
    )

    overview = (
        alt.Chart(long)
          .mark_area(opacity=0.25)
          .encode(
              x=alt.X("ts_min:T", title=None),
              y=alt.Y("price:Q", title=None, aggregate="mean"),
              color=alt.Color("EnvState:N", legend=None)
          )
          .add_params(brush)
          .properties(height=70)
    )

    st.altair_chart((detail & overview).resolve_scale(color='shared'), use_container_width=True)

# ---------------- íˆíŠ¸ë§µ(ì˜µì…˜ í¬í•¨) ----------------
def heatmap_fixed_axes(df: pd.DataFrame, freq: str, tol: float = 0.0,
                       scale_mode: str = "Absolute Frequency",
                       show_values: bool = True):
    """
    ê³ ì • ì¶• íˆíŠ¸ë§µ + ì˜µì…˜
      - tol: ë™ë¥  í—ˆìš© ì˜¤ì°¨ (ê°™ì€ ì‹œê°ì— ê°€ê²© ì°¨ì´ê°€ tol ì´ë‚´ë©´ ëª¨ë‘ ìµœì €ê°€ë¡œ ì§‘ê³„)
      - scale_mode:
          * Absolute Frequency                : ì ˆëŒ€ íšŸìˆ˜
          * Relative Frequency (Global %)     : ì „ì²´ ëŒ€ë¹„ ë°±ë¶„ìœ¨
          * Region-normalized Proportion      : ì§€ì—­(KR/US) ë‚´ ë¹„ìœ¨
          * Device-normalized Proportion      : ë””ë°”ì´ìŠ¤(PC/Mobile) ë‚´ ë¹„ìœ¨
      - show_values: ì…€ ê°’ í‘œì‹œ ì—¬ë¶€
    """
    need_any = {"ts_min","price","region","device"}
    if df is None or df.empty or not need_any.issubset(df.columns):
        st.info("íˆíŠ¸ë§µì„ ê·¸ë¦´ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return

    t = df.copy()
    t["ts_min"] = pd.to_datetime(t["ts_min"], errors="coerce")
    t = t.dropna(subset=["ts_min","price"]).sort_values("ts_min")
    if t.empty:
        st.info("íˆíŠ¸ë§µì„ ê·¸ë¦´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return

    # Region/Device ì •ê·œí™” â†’ KR/US, PC/Mobile
    t["region"] = t["region"].astype(str).str.upper().replace({"KOREA":"KR","USA":"US"})
    dv = t["device"].astype(str).str.lower()
    t["device_std"] = np.where(dv.str.contains("mobile"), "Mobile",
                        np.where(dv.str.contains("pc|desktop"), "PC", dv.str.title()))
    t["env_label"] = t["region"] + "-" + t["device_std"]

    x_order = ["KR-PC","KR-Mobile","US-PC","US-Mobile"]
    x_vals = [x for x in x_order if x in t["env_label"].unique().tolist()]
    if not x_vals:
        st.info("í‘œì‹œí•  í™˜ê²½ ì¡°í•©(KR/US Ã— PC/Mobile)ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì‚¬ìš©í•  Yì¶• êµ¬ì„± (ìˆì„ ë•Œë§Œ)
    y_blocks = []
    if "logged_in" in t.columns or "login" in t.columns:
        col = "logged_in" if "logged_in" in t.columns else "login"
        y_blocks.append((col, ["L1","L0"], "Login"))
    if "cart" in t.columns:
        y_blocks.append(("cart", ["Cart","NoCart"], "Cart"))
    if "cookie_cleared" in t.columns:
        y_blocks.append(("cookie_cleared", ["Cleared","NotCleared"], "Cookie"))
    if not y_blocks:
        st.info("í‘œì‹œí•  Yì¶• ìƒíƒœ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (login/logged_in, cart, cookie_cleared)")
        return

    # Yì¶• ì „ì²´ ìˆœì„œ
    y_vals = []
    for col, vals, prefix in y_blocks:
        for v in vals:
            y_vals.append(f"{prefix}={v}")

    def count_min(flag_col: str, vals: list, prefix: str) -> pd.DataFrame:
        sub = t.copy()
        sub["row_label"] = prefix + "=" + sub[flag_col].astype(str)
        sub = sub[sub["row_label"].isin([f"{prefix}={v}" for v in vals])]
        if sub.empty:
            return pd.DataFrame(columns=["row_label","env_label","count"])
        sub["col_key"] = sub["env_label"] + " | " + sub["row_label"]
        wide = (sub.pivot_table(index="ts_min", columns="col_key", values="price", aggfunc="mean")
                   .resample(freq).mean())
        if wide is None or wide.empty:
            return pd.DataFrame(columns=["row_label","env_label","count"])
        wmin = wide.min(axis=1)
        is_tie = (wide.sub(wmin, axis=0).abs() <= float(tol))
        counts = is_tie.sum(axis=0).rename("count").rename_axis("col_key").reset_index()
        parts = counts["col_key"].str.split(r" \| ", n=1, expand=True)
        counts["env_label"] = parts[0]
        counts["row_label"] = parts[1]
        return counts[["row_label","env_label","count"]]

    blocks = [count_min(col, vals, prefix) for (col, vals, prefix) in y_blocks]
    blocks = [b for b in blocks if b is not None and not b.empty]
    if not blocks:
        st.info("ì§‘ê³„í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."); return
    counts = pd.concat(blocks, ignore_index=True)

    full = pd.MultiIndex.from_product([y_vals, x_vals], names=["row_label","env_label"])
    counts = counts.set_index(["row_label","env_label"]).reindex(full, fill_value=0).reset_index()

    # ìŠ¤ì¼€ì¼ ë³€í™˜
    parts = counts["env_label"].str.split("-", n=1, expand=True)
    counts["region"] = parts[0]
    counts["device"] = parts[1]

    value_col = "value"
    color_title = "ìµœì €ê°€ íšŸìˆ˜"
    text_fmt = "d"

    if scale_mode == "Absolute Frequency":
        counts[value_col] = counts["count"].astype(float)
        domain = [0.0, float(counts[value_col].max() or 1.0)]
        text_fmt = "d"
    elif scale_mode == "Relative Frequency (Global %)":
        total = float(counts["count"].sum()) or 1.0
        counts[value_col] = (counts["count"] / total) * 100.0
        color_title = "Global %"
        domain = [0.0, 100.0]
        text_fmt = ".1f"
    elif scale_mode == "Region-normalized Proportion":
        totals = counts.groupby("region")["count"].transform("sum").replace(0, np.nan)
        counts[value_col] = counts["count"] / totals
        color_title = "Proportion (by Region)"
        domain = [0.0, 1.0]
        text_fmt = ".2f"
    else:  # Device-normalized Proportion
        totals = counts.groupby("device")["count"].transform("sum").replace(0, np.nan)
        counts[value_col] = counts["count"] / totals
        color_title = "Proportion (by Device)"
        domain = [0.0, 1.0]
        text_fmt = ".2f"

    st.subheader("Lowest Price by Environment Combination")
    base = (
        alt.Chart(counts)
          .mark_rect(cornerRadius=6)
          .encode(
              x=alt.X("env_label:N", title="Environment", sort=x_order,
                      scale=alt.Scale(paddingInner=0.15, paddingOuter=0.05),
                      axis=alt.Axis(labelAngle=0)),
              y=alt.Y("row_label:N", title="Login / Cart / Cookie", sort=y_vals,
                      scale=alt.Scale(paddingInner=0.2, paddingOuter=0.1)),
              color=alt.Color(f"{value_col}:Q", title=color_title,
                              scale=alt.Scale(domain=domain, scheme="blues")),
              tooltip=[
                  alt.Tooltip("row_label:N", title="Row"),
                  alt.Tooltip("env_label:N", title="Env"),
                  alt.Tooltip("count:Q", title="ìµœì €ê°€ íšŸìˆ˜", format="d"),
                  alt.Tooltip(f"{value_col}:Q", title=color_title,
                              format=(".2f" if "Proportion" in color_title else (".1f" if "Global" in color_title else "d"))),
              ]
          )
          .properties(height=340)
    )

    chart = base
    if show_values:
        labels = (
            alt.Chart(counts)
              .mark_text(baseline="middle", fontSize=12, fontWeight="bold")
              .encode(
                  x="env_label:N", y="row_label:N",
                  text=alt.Text(f"{value_col}:Q", format=text_fmt),
                  color=alt.condition(
                      alt.datum[value_col] > (domain[1]*0.6 if domain[1] else 0),
                      alt.value("white"), alt.value("black")
                  )
              )
              .properties(height=340)
        )
        chart = base + labels

    st.altair_chart(
        chart.resolve_scale(color='shared').configure_axis(grid=False).configure_view(strokeOpacity=0),
        use_container_width=True
    )

# ---------------- main ----------------
def main():
    try:
        df_all = load_all_data()
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.stop()

    st.title("ğŸ’¹ Real-Time Visualization")

    cfg = sidebar_controls(df_all)
    view = cfg["view"]
    data = apply_filters(view, cfg)

    st.subheader(f"[{cfg['site']}] {cfg['item']} â€” {cfg['start_ts']:%Y-%m-%d %H:%M} ì´í›„")
    st.caption(f"Env: {', '.join(cfg['envs']) if cfg['envs'] else '(ì—†ìŒ)'} Â· ì†ŒìŠ¤ íŒŒì¼ ìˆ˜: {len(view['source'].unique()) if 'source' in view.columns else '-'}")

    kpi_row(data)
    st.divider()

    st.subheader("Price Over Time")
    plot_env_state_lines(data, freq=cfg["freq"])

    st.divider()
    with st.expander("Options", expanded=False):
        tie_tol = st.number_input("ë™ë¥  í—ˆìš© ì˜¤ì°¨", min_value=0.0, value=0.00, step=0.01, format="%.2f")
        scale_mode = st.radio(
            "Color Scale",
            options=[
                "Absolute Frequency",
                "Relative Frequency (Global %)",
                "Region-normalized Proportion",
                "Device-normalized Proportion",
            ],
            index=0,
        )
        show_values = st.checkbox("Show Values", value=True)

    heatmap_fixed_axes(
        data, cfg["freq"],
        tol=tie_tol,
        scale_mode=scale_mode,
        show_values=show_values,
    )

    with st.expander("ì›ë³¸ ë°ì´í„° ë³´ê¸°", expanded=False):
        cols = ["source","state","site","item","env","region","device","logged_in","login","cart","cookie_cleared","ts_min","price","currency"]
        cols = [c for c in cols if c in data.columns]
        show = data.copy()
        if "ts_min" in show.columns:
            show["ts_min"] = pd.to_datetime(show["ts_min"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M:%S")
        st.dataframe(show[cols] if cols else show, use_container_width=True, height=700)
        csv = show.to_csv(index=False).encode("utf-8")
        st.download_button("CSV download", data=csv, file_name="raw_data.csv", mime="text/csv")

    if st.button("ìƒˆë¡œê³ ì¹¨"):
        st.rerun()

if __name__ == "__main__":
    main()
